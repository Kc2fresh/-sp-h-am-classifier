{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was inspired by eduonix and kavgan. it  is a text message spam\\ham classifier. it achieves a 98.4 % accuracy, the preprocessing and processing stages will be refined in the near future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:{} 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n",
      "NLTK:{} 3.4.5\n",
      "Pandas:{} 0.25.1\n",
      "Scikit-learn:{} 0.21.3\n",
      "Numpy:{} 1.16.5\n"
     ]
    }
   ],
   "source": [
    "#calling dependencies\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy\n",
    "\n",
    "print('Python:{}',format(sys.version))\n",
    "print('NLTK:{}',format(nltk.__version__))\n",
    "print('Pandas:{}',format(pandas.__version__))\n",
    "print('Scikit-learn:{}',format(sklearn.__version__))\n",
    "print('Numpy:{}',format(numpy.__version__))\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the Dataset(dataset downloaded from Kaggle https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#load  dataset in workable format\n",
    "\n",
    "df = pd.read_table('SMSSpamCollection',header = None, encoding = 'utf-8')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "0    5572 non-null object\n",
      "1    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n",
      "None\n",
      "      0                                                  1\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#basic info about the dataset\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(\n",
    ")\n",
    "classes = df[0]\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert class labels to binary values 0=ham, 1= spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     ham\n",
      "1     ham\n",
      "2    spam\n",
      "3     ham\n",
      "4     ham\n",
      "5    spam\n",
      "6     ham\n",
      "7     ham\n",
      "8    spam\n",
      "9    spam\n",
      "Name: 0, dtype: object\n",
      "[0 0 1 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n",
    "print(classes[:10])\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store sms message data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "5    FreeMsg Hey there darling it's been 3 week's n...\n",
      "6    Even my brother is not like to speak with me. ...\n",
      "7    As per your request 'Melle Melle (Oru Minnamin...\n",
      "8    WINNER!! As a valued network customer you have...\n",
      "9    Had your mobile 11 months or more? U R entitle...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sms_messages = df[1]\n",
    "print(sms_messages[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use regexp to replace email addresses, url's, phone numbers, other numbers, symbols\n",
    "\n",
    "replace email addresses with 'emailadd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ue regexp to replace urls, email addresses and currencies\n",
    "processed = sms_messages.str.replace(r'^,+@[^\\,],*\\,[a-z]{2,}$', 'emilad')  \n",
    "\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\,]+\\,[a-zA-Z]{2,3}(/\\S*)?$', 'wbadd')\n",
    "\n",
    "processed = processed.str.replace(r'£|\\$', 'cshsgn')\n",
    "\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','pnumbr')\n",
    "                                  \n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'nber')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go until jurong point crazy available only in ...\n",
      "1                                 ok lar joking wif u oni\n",
      "2       free entry in nber a wkly comp to win fa cup f...\n",
      "3             u dun say so early hor u c already then say\n",
      "4       nah i dont think he goes to usf he lives aroun...\n",
      "                              ...                        \n",
      "5567    this is the nbernd time we have tried nber con...\n",
      "5568                  will ü b going to esplanade fr home\n",
      "5569    pity  was in mood for that soany other suggest...\n",
      "5570    the guy did some bitching but i acted like id ...\n",
      "5571                            rofl its true to its name\n",
      "Name: 1, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation\n",
    "\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]','')\n",
    "\n",
    "#change to lower case\n",
    "processed = processed.str.lower()\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and remove stop words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#creating bag of words\n",
    "\n",
    "all_words = []\n",
    "\n",
    "\n",
    "for message in processed:\n",
    "    sentences = sent_tokenize(message)\n",
    "    for words in sentences:\n",
    "        w= word_tokenize(words)\n",
    "        for c in w:\n",
    "            all_words.append(c)\n",
    "    \n",
    "all_words = FreqDist(all_words)\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: {} 8740\n",
      "Most common words: {} [('go', 1), ('jurong', 1), ('point', 1), ('crazy', 1), ('available', 1), ('bugis', 1), ('n', 1), ('great', 1), ('world', 1), ('la', 1), ('e', 1), ('buffet', 1), ('cine', 1), ('got', 1), ('amore', 1), ('wat', 1), ('ok', 1), ('lar', 1), ('joking', 1), ('wif', 1), ('u', 1), ('oni', 1), ('free', 1), ('entry', 1), ('nber', 1), ('wkly', 1), ('comp', 1), ('win', 1), ('fa', 1), ('cup', 1), ('final', 1), ('tkts', 1), ('nberst', 1), ('may', 1), ('text', 1), ('receive', 1), ('questionstd', 1), ('txt', 1), ('ratetcs', 1), ('apply', 1), ('nberovernbers', 1), ('dun', 1), ('say', 1), ('early', 1), ('hor', 1), ('c', 1), ('already', 1), ('nah', 1), ('dont', 1), ('think', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "temp_processed=[token for token in all_words if token not in stop_words]\n",
    "temp_processed = FreqDist(temp_processed)\n",
    "print('number of words: {}',format(len(temp_processed)))\n",
    "print('Most common words: {}',format(temp_processed.most_common(50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "until\n",
      "jurong\n",
      "point\n",
      "crazy\n",
      "available\n",
      "only\n",
      "in\n",
      "bugis\n",
      "n\n",
      "great\n",
      "world\n",
      "la\n",
      "e\n",
      "buffet\n",
      "cine\n",
      "there\n",
      "got\n",
      "amore\n",
      "wat\n"
     ]
    }
   ],
   "source": [
    "#use the most common features to train\n",
    "word_features = list(all_words.keys())[:1500]\n",
    "#define a find features function\n",
    "def find_features(message):\n",
    "    words=word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "#view results\n",
    "\n",
    "features = find_features(processed[0])\n",
    "for key,value in features.items():\n",
    "    if value == True:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find features for all messages\n",
    "messages = list(zip(processed, Y))\n",
    "\n",
    "\n",
    "#define seed for reproducibility\n",
    "seed=1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "featuresets = [(find_features(text),label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the featuresets into training and testing\n",
    "from sklearn import model_selection\n",
    "\n",
    "#split the data into training and testing\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size =0.20, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457\n",
      "1115\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn Classifiers with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy: 98.11659192825111\n"
     ]
    }
   ],
   "source": [
    "#we will import some algorithms from sklearn to use as well as some performance metrics\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model= SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "\n",
    "#train the model\n",
    "model.train(training)\n",
    "\n",
    "#test on testing dataset\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "\n",
    "print(\"SVC accuracy: {}\". format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 95.96412556053812\n",
      "Decision Tree Accuracy: 94.43946188340807\n",
      "Random Forest Accuracy: 98.65470852017937\n",
      "Logistic Regression Accuracy: 97.57847533632287\n",
      "SGD Classifier Accuracy: 98.02690582959642\n",
      "Naive Bayes Accuracy: 98.20627802690582\n",
      "SVM Linear Accuracy: 98.11659192825111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "#define models to train\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \n",
    "         \"SGD Classifier\", \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "classifiers= [\n",
    "    DecisionTreeClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    MultinomialNB(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model =SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy : 98.11659192825111\n"
     ]
    }
   ],
   "source": [
    "#ensemble voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \n",
    "         \"SGD Classifier\", \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers= [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models,voting ='hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify. accuracy(nltk_model, testing)*100\n",
    "print(\"Voting Classifier: Accuracy : {}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make class label for testing set\n",
    "testing_features, labels = list(zip(*testing))\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(testing_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       970\n",
      "           1       1.00      0.89      0.94       145\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.99      0.94      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"2\" valign=\"top\">actual</td>\n",
       "      <td>ham</td>\n",
       "      <td>970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spam</td>\n",
       "      <td>16</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham        970    0\n",
       "       spam        16  129"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print a confusion matrix and classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
